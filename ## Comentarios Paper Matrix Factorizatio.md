## Comentarios Paper Matrix Factorization Techniques for Recommender Systems

El paper hecho por Yehuda Koren, Robert Bell y Chris Volinsky explica una nueva forma de sistemas recomendadores que habíamos vistos hasta hoy en día. Hasta este momento nos habiamos basado en sistemas que usaban lo k-vecinos más cercanos tanto para items como para usuarios, pero ambos por separados. Es en esto específicamente que se hace un quiebre y vemos algo nuevo en donde ahora no se usan ambos por separados, sino más bien que se usan ambas informaciones en conjunto para poder lograr tener este nuevo sistema recomendador. Para poder hacer esto, ellos explicany describen el uso de factores. Estos quieren decir que son cualidades que describen tanto al usuario como a las películas de donde coloquialmente uno puede entender desde dónde proviene la similitud o el punto de encuentro que se genera entre los usuarios y las peliculas en base a estos factores. De esta manera podemos ver que si describimos a ciertas peliculas y usuarios en base a dos dimensiones en un plano X - Y, es que los vectores que se generan para una pelicula y un usuario, entre menor es el angulo entre ellos, más posible es que la recomendación debería existir, obviamente con toda una base matemática por detrás. Esto nos ayuda en un gran problema que sucede en las matrices que se forman entre usuarios e items donde es muy poco probable que un usuario haya dado "ratings" para todos los distintos items por lo que hay información que falta, pero esta forma de factorizar matrices ayuda a la incorporación de información adicional mediante información implícita.

A lo largo del paper algo que rescato es como fueron "complicando" o haciendo cada vez más realista la predicción. Esto es debido a que en un principo se parte con una simple minimización del problem entre el valor real del rating y la predicción hecha meramente por el producto punto de vectores entre usuarios e items, pero a medida que avanza el paper empieza a añadir complejidades de la vida real, tales como los bias que pueden haber, añadir información implícita, añadir el hecho de que las preferencias por algún item de una persona pueden ir cambiando con el tiempo o que inclusive hay items que tienen una cierta popularidad estacionaria. Recalco el hecho de que lo encuentro una muy buena metodología de enseñanza para entenderlo de manera correcta.

Por otro parte, a pesar de explicar todo de manera sencilla, me queda la sensación de vacio en la parte más práctica. Viendo que es un paper de una materia que no es trivial a mi parecer, a pesar de que entendí el transfondo, hubiera sido de mucha ayuda algún ejemplo más práctico, evidenciando tambien cómo es que funcionan por detrás los "learning algorithms". Junto a esto algo que creo que podría haber servido es ver ien cómo escoger los factores que van a describir los items y usuarios. Esto debidioa a que claramente uno tiene que tener buenos factores que caractericen tanto usuarios como items, por lo que ver un ejemplo de qué sería un buen "approach" hubiera sido de bastante ayuda.

Por último, algo que no me queda para nada claro es cómo reconocen la información implícita. Hablan de N(u) lo que son un set de items que se tiene información implícita, pero nunca hablan de qué significa este booleano y sumado a eso hacen una sumatoria en base a X_i donde i pertenece a este conjunto de items donde se tiene información implícita, pero nunca hablan y me queda la gran duda, qué es ese x, qué significa sobre ese atrbituto implícito. Estas son algunas de las preguntas que me quedan inconclusas de este paper, el cual a pesar de que explica bien el transfondo teórico y lo que se quiere lograr, encuentro que queda pobre a la hora de explicar la parte más práctica la cual es la que finalmente uno neceista entender para llevar esto a cabo.
